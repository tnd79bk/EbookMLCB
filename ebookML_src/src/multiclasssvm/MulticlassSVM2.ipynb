{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape       =  (49000, 3073)\n",
      "X_val shape         =  (1000, 3073)\n",
      "X_test shape        =  (10000, 3073)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "# Load CIFAR 10 dataset\n",
    "cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# Tạo một tập validation từ X_train \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size= 1000)\n",
    "# print('(New) X_train shape = ', X_train.shape)\n",
    "# print('X_val shape         = ', X_val.shape)\n",
    "\n",
    "# Feature engineering \n",
    "# mean image of all training images\n",
    "img_mean = np.mean(X_train, axis = 0)\n",
    "\n",
    "def feature_engineering(X):\n",
    "    X -= img_mean # zero-centered\n",
    "    N = X.shape[0] # number of data point \n",
    "    X = X.reshape(N, -1) # vectorizetion \n",
    "    return np.concatenate((X, np.ones((N, 1))), axis = 1) # bias trick \n",
    "\n",
    "X_train = feature_engineering(X_train)\n",
    "X_val = feature_engineering(X_val)\n",
    "X_test = feature_engineering(X_test)\n",
    "\n",
    "print('X_train shape       = ', X_train.shape)\n",
    "print('X_val shape         = ', X_val.shape)\n",
    "print('X_test shape        = ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idx = np.random.randint(0, X_train.shape[0], replace = False)\n",
    "n_dev = 10\n",
    "X_dev = X_train[:n_dev]\n",
    "y_dev = y_train[:n_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with reg = 0  : 35.28762795657478\n",
      "Loss with reg = 0.1: 479.8668472462361\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# naive way to calculate loss and grad\n",
    "def svm_loss_naive(W, X, y, reg):\n",
    "    ''' calculate loss and gradient of the loss function at W\n",
    "    W: 2d numpy array of shape (d, C). The weight matrix.\n",
    "    X: 2d numpy array of shape (N, d). The training data \n",
    "    y: 1d numpy array of shape (N,). The training label\n",
    "    reg: a positive number. The regularization parameter\n",
    "    '''\n",
    "    d, C = W.shape \n",
    "    N = X.shape[0]\n",
    "    \n",
    "    ## naive loss and grad\n",
    "    loss = 0 \n",
    "    dW = np.zeros_like(W)\n",
    "    for n in xrange(N):\n",
    "        xn = X[n]\n",
    "        score = xn.dot(W)\n",
    "        for j in xrange(C):\n",
    "            if j == y[n]:\n",
    "                continue \n",
    "            margin = 1 - score[y[n]] + score[j]\n",
    "            if margin > 0:\n",
    "                loss += margin \n",
    "                dW[:, j] += xn \n",
    "                dW[:, y[n]] -= xn\n",
    "    \n",
    "    loss /= N \n",
    "    loss += 0.5*reg*np.sum(W * W) \n",
    "    dW /= N \n",
    "    dW += reg*W\n",
    "    return loss, dW\n",
    "    \n",
    "# random, small data\n",
    "d, C, N = 3073, 3, 10\n",
    "reg = .1 \n",
    "W_rand = np.random.randn(d, C)\n",
    "X_rand = np.random.randn(N, d)\n",
    "y_rand = np.random.randint(0, C, N)\n",
    "\n",
    "# sanity check\n",
    "print('Loss with reg = 0  :', svm_loss_naive(W_rand, X_rand, y_rand, 0)[0])\n",
    "print('Loss with reg = 0.1:',svm_loss_naive(W_rand, X_rand, y_rand, .1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient difference: 0.000002\n"
     ]
    }
   ],
   "source": [
    "f = lambda W: svm_loss_naive(W, X_rand, y_rand, .1)[0]\n",
    "\n",
    "# for checking if calculated grad is correct\n",
    "def numerical_grad_general(W, f):\n",
    "    eps = 1e-6\n",
    "    g = np.zeros_like(W)\n",
    "    # flatening variable -> 1d. Then we need \n",
    "    # only one for loop\n",
    "    W_flattened = W.flatten()\n",
    "    g_flattened = np.zeros_like(W_flattened)\n",
    "    \n",
    "    for i in xrange(W.size):\n",
    "        W_p = W_flattened.copy()\n",
    "        W_n = W_flattened.copy()\n",
    "        W_p[i] += eps \n",
    "        W_n[i] -= eps \n",
    "        \n",
    "        # back to shape of W \n",
    "        W_p = W_p.reshape(W.shape)\n",
    "        W_n = W_n.reshape(W.shape)\n",
    "        g_flattened[i] = (f(W_p) - f(W_n))/(2*eps)\n",
    "        \n",
    "    # convert back to original shape\n",
    "    return g_flattened.reshape(W.shape) \n",
    "\n",
    "# compare two ways of computing gradient\n",
    "g1 = svm_loss_naive(W_rand, X_rand, y_rand, .1)[1]\n",
    "g2 = numerical_grad_general(W_rand, f)\n",
    "print('gradient difference: %f' %np.linalg.norm(g1 - g2)) # this should be very small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more efficient way to compute loss and grad\n",
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "    d, C = W.shape \n",
    "    N = X.shape[0]\n",
    "    loss = 0 \n",
    "    dW = np.zeros_like(W)\n",
    "    \n",
    "    Z = X.dot(W) # shape of (N, C)\n",
    "    id0 = np.arange(Z.shape[0])\n",
    "    correct_class_score = Z[id0, y].reshape(N, 1) # shape of (N, 1)\n",
    "    margins = np.maximum(0, Z - correct_class_score + 1) # shape of (N, C)\n",
    "    margins[id0, y] = 0\n",
    "    loss = np.sum(margins)\n",
    "    loss /= N \n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    \n",
    "    F = (margins > 0).astype(int)# shape of (N, C)\n",
    "    F[np.arange(F.shape[0]), y] = np.sum(-F, axis = 1)\n",
    "    dW = X.T.dot(F)/N + reg*W\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive      -- run time: 7.02469491959 (s)\n",
      "Vectorized -- run time: 0.443039894104 (s)\n",
      "loss difference: 1.7826096154749393e-10\n",
      "gradient difference: 1.9876779340328013e-10\n"
     ]
    }
   ],
   "source": [
    "d, C = 3073, 10\n",
    "reg = .1 \n",
    "W_rand = np.random.randn(d, C)\n",
    "import time \n",
    "t1 = time.time()\n",
    "l1, dW1 = svm_loss_naive(W_rand, X_train, y_train, reg)\n",
    "t2 = time.time()\n",
    "print('Naive      -- run time:', t2 - t1, '(s)')\n",
    "\n",
    "t1 = time.time()\n",
    "l2, dW2 = svm_loss_vectorized(W_rand, X_train, y_train, reg)\n",
    "t2 = time.time()\n",
    "print('Vectorized -- run time:', t2 - t1, '(s)')\n",
    "print('loss difference:', np.linalg.norm(l1 - l2))\n",
    "print('gradient difference:', np.linalg.norm(dW1 - dW2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 3073)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5/50, loss = 5.518609\n",
      "epoch 10/50, loss = 5.433403\n",
      "epoch 15/50, loss = 4.934789\n",
      "epoch 20/50, loss = 4.773220\n",
      "epoch 25/50, loss = 4.866825\n",
      "epoch 30/50, loss = 4.678948\n",
      "epoch 35/50, loss = 4.475465\n",
      "epoch 40/50, loss = 4.868525\n",
      "epoch 45/50, loss = 4.622545\n"
     ]
    }
   ],
   "source": [
    "# Mini-batch gradient descent\n",
    "def multiclass_svm_GD(X, y, Winit, reg, lr=.1, \\\n",
    "        batch_size = 1000, num_iters = 50, print_every = 10):\n",
    "    W = Winit \n",
    "    loss_history = []\n",
    "    for it in xrange(num_iters):\n",
    "        mix_ids = np.random.permutation(X.shape[0])\n",
    "        n_batches = int(np.ceil(X.shape[0]/float(batch_size)))\n",
    "        for ib in range(n_batches):\n",
    "            ids = mix_ids[batch_size*ib: min(batch_size*(ib+1), X.shape[0])]\n",
    "            X_batch = X[ids]\n",
    "            y_batch = y[ids]\n",
    "            lossib, dW = svm_loss_vectorized(W, X_batch, y_batch, reg)\n",
    "            loss_history.append(lossib)\n",
    "            W -= lr*dW \n",
    "        if it % print_every == 0 and it > 0:\n",
    "           print('epoch %d/%d, loss = %f' %(it, num_iters, loss_history[-1]))\n",
    "    return W, loss_history \n",
    "\n",
    "d, C = X_train.shape[1], 10\n",
    "reg = .1 \n",
    "W = 0.00001*np.random.randn(d, C)\n",
    "\n",
    "W, loss_history = multiclass_svm_GD(X_train, y_train, W, reg, lr = 1e-8, num_iters = 50, print_every = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAENCAYAAADnrmWtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbo1CXKmZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# plot loss as a function of iteration\n",
    "with PdfPages('loss_history.pdf') as pdf:\n",
    "    plt.plot(loss_history, 'k')\n",
    "    plt.xlabel('number of iterations', fontsize = 13)\n",
    "    plt.ylabel('loss function', fontsize = 13)\n",
    "    pdf.savefig()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multisvm_predict(W, X):\n",
    "    Z = X.dot(W)\n",
    "    return np.argmax(Z, axis=1)\n",
    "\n",
    "def evaluate(W, X, y):\n",
    "    y_pred = multisvm_predict(W, X)\n",
    "    acc = 100*np.mean(y_pred == y)\n",
    "    return acc \n",
    "# y_pred = multisvm_predict(W, X_test)\n",
    "# acc = 100*np.mean(y_pred == y_test)\n",
    "# print('training accuracy: %.2f %%' % acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 1.000000e-09, reg = 1.000000e-01, loss = 4.422479, validation acc = 40.30\n",
      "lr = 1.000000e-09, reg = 1.000000e-02, loss = 4.474095, validation acc = 40.70\n",
      "lr = 1.000000e-09, reg = 1.000000e-03, loss = 4.240144, validation acc = 40.90\n",
      "lr = 1.000000e-09, reg = 1.000000e-04, loss = 4.257436, validation acc = 41.40\n",
      "lr = 1.000000e-08, reg = 1.000000e-01, loss = 4.482856, validation acc = 41.50\n",
      "lr = 1.000000e-08, reg = 1.000000e-02, loss = 4.036566, validation acc = 41.40\n",
      "lr = 1.000000e-08, reg = 1.000000e-03, loss = 4.085053, validation acc = 41.00\n",
      "lr = 1.000000e-08, reg = 1.000000e-04, loss = 3.891934, validation acc = 41.40\n",
      "lr = 1.000000e-07, reg = 1.000000e-01, loss = 3.947408, validation acc = 41.50\n",
      "lr = 1.000000e-07, reg = 1.000000e-02, loss = 4.088984, validation acc = 41.90\n",
      "lr = 1.000000e-07, reg = 1.000000e-03, loss = 4.073365, validation acc = 41.70\n",
      "lr = 1.000000e-07, reg = 1.000000e-04, loss = 4.006863, validation acc = 41.80\n",
      "lr = 1.000000e-06, reg = 1.000000e-01, loss = 3.851727, validation acc = 41.90\n",
      "lr = 1.000000e-06, reg = 1.000000e-02, loss = 3.941015, validation acc = 41.80\n",
      "lr = 1.000000e-06, reg = 1.000000e-03, loss = 3.995598, validation acc = 41.60\n",
      "lr = 1.000000e-06, reg = 1.000000e-04, loss = 3.857822, validation acc = 41.80\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "regs = [0.1, 0.01, 0.001, 0.0001]\n",
    "best_W = 0\n",
    "best_acc = 0\n",
    "for lr in lrs:\n",
    "    for reg in regs: \n",
    "        W, loss_history = multiclass_svm_GD(X_train, y_train, W, reg, \\\n",
    "                                            lr = 1e-8, num_iters = 100, print_every = 1e20)\n",
    "        acc = evaluate(W, X_val, y_val)\n",
    "        print('lr = %e, reg = %e, loss = %f, validation acc = %.2f' %(lr, reg, loss_history[-1], acc))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc \n",
    "            best_W = W \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3073, 10)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data =  39.88\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(best_W, X_test, y_test)\n",
    "print('Accuracy on test data = %2f %%'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 1.000000e-07, reg = 1.000000e-04, loss = 4.432414, validation acc = 39.90\n",
      "lr = 1.000000e-07, reg = 1.000000e-05, loss = 4.110013, validation acc = 40.10\n",
      "lr = 1.000000e-07, reg = 1.000000e-06, loss = 4.212654, validation acc = 40.80\n",
      "lr = 1.000000e-06, reg = 1.000000e-04, loss = 4.240339, validation acc = 41.30\n",
      "lr = 1.000000e-06, reg = 1.000000e-05, loss = 4.295307, validation acc = 41.00\n",
      "lr = 1.000000e-06, reg = 1.000000e-06, loss = 4.209549, validation acc = 41.20\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-7, 1e-6]\n",
    "regs = [0.0001, 1e-5, 1e-6]\n",
    "best_W = 0\n",
    "best_acc = 0\n",
    "for lr in lrs:\n",
    "    for reg in regs: \n",
    "        W, loss_history = multiclass_svm_GD(X_train, y_train, W, reg, \\\n",
    "                                            lr = 1e-8, num_iters = 100, print_every = 1e20)\n",
    "        acc = evaluate(W, X_val, y_val)\n",
    "        print('lr = %e, reg = %e, loss = %f, validation acc = %.2f' %(lr, reg, loss_history[-1], acc))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc \n",
    "            best_W = W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 %\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "print('%d %%' %a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
