{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giới thiệu\n",
    "\n",
    "Trong bài này, tôi sẽ giới thiệu thuật toán đầu tiên trong Classification có tên là Perceptron Learning Algorithm (PLA) hoặc đôi khi được viết gọn là Perceptron. \n",
    "\n",
    "Perceptron là một thuật toán Classification cho trường hợp đơn giản nhất: chỉ có hai class (lớp) và cũng chỉ hoạt động được trong một trường hợp rất cụ thể. Tuy nhiên, nó là nền tảng cho một mảng lớn quan trọng của Machine Learning là Neural Networks và sau này là Deep Learning. Easy first! (Tại sao lại gọi là Neural Networks - tức mạng dây thần kinh - các bạn sẽ được thấy ở cuối bài).\n",
    "\n",
    "Giải sử chúng ta có hai tập hợp dữ liệu đã được gán nhãn được minh hoạ như Hình 1 bên trái dưới đây. Hai tập hợp của chúng ta là tập các điểm màu xanh và tập các điểm màu đỏ. Bài toán đặt ra là: từ dữ liệu của hai tập được gán nhãn chúng ta có, hãy xây dựng một _classifier_ (bộ phân lớp) để khi có một điểm dữ liệu hình tam giác màu xám mới, ta có thể dự đoán được màu (nhãn) của nó. Mỗi tập/nhóm dữ liệu đã được gán nhãn được gọi là một _class_.\n",
    "\n",
    "<table width = \"100%\" style = \"border: 0px solid white\">\n",
    "   <tr >\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\"> \n",
    "        <img style=\"display:block;\" width = \"100%\" src = \"/assets/pla/pla1.png\">\n",
    "         </td>\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\">\n",
    "        <img style=\"display:block;\" width = \"100%\" src = \"/assets/pla/pla2.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "Hiểu theo một cách khác, chúng ta cần tìm _lãnh thổ_ của mỗi class sao cho, với mỗi một điểm mới, ta chỉ cần xác định xem nó nằm vào lãnh thổ của class nào rồi quyết định nó thuộc class đó. Để tìm _lãnh thổ_ của mỗi class, chúng ta cần đi tìm biên giới (boundary) giữa hai _lãnh thổ_ này. Vậy bài toán classification có thể coi là bài toán đi tìm boundary giữa các class. Và boundary đơn giản nhât trong không gian hai chiều là một đường thằng, trong không gian ba chiều là một mặt phẳng, trong không gian nhiều chiều là một siêu mặt phẳng (hyperplane) (tôi gọi chung những boundary này là _đường phẳng_). Những boundary phẳng này được coi là đơn giản vì nó có thể biểu diễn dưới dạng toán học bằng một hàm số đơn giản có dạng tuyến tính, tức linear. Tất nhiên, chúng ta đang giả sử một cách đơn giản rằng tồn tại một đường phẳng để có thể phân định _lãnh thổ_ của hai class. Hình 1 bên phải minh họa một đường thẳng phân chia hai class trong mặt phẳng. Phần có nền màu xanh được coi là _lãnh thổ_ của lớp xanh, phần có nên màu đỏ được coi là _lãnh thổ_ của lớp đỏ. Trong trường hợp này, điểm dữ liệu mới hình tam giác được phân vào class đỏ. \n",
    "\n",
    "### Bài toán Perceptron \n",
    "Bài toán Perceptron được phát biểu như sau: _Cho hai class được gán nhãn, hãy tìm một đường phẳng sao cho toàn bộ các điểm thuộc class 1 nằm về 1 phía, toàn bộ các điểm thuộc class 2 nằm về phía còn lại của đường phẳng đó. Với giả định rằng tồn tại một đường phẳng như thế._\n",
    "\n",
    "Nếu tồn tại một đường phẳng phân chia hai class thì ta gọi hai class đó là _linear separable_. \n",
    "\n",
    "## Thuật toán Perceptron (PLA)\n",
    "Cũng giống như các thuật toán lặp trong [K-means Clustering](/2017/01/01/kmeans/) và [Gradient Descent](/2017/01/12/gradientdescent/), ý tưởng cơ bản của PLA là xuất phát từ một nghiệm dự đoán nào đó, qua mỗi vòng lặp, nghiệm sẽ được cập nhật tới một ví trí tốt hơn. Việc cập nhật này dựa trên việc giảm giá trị của một hàm mất mát nào đó. \n",
    "\n",
    "### Một số ký hiệu\n",
    "Giả sử \\\\(\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N] \\in \\mathbb{R}^{d \\times N}\\\\) là ma trận chứa các điểm dữ liệu mà mỗi cột \\\\(\\mathbf{x}_i \\in \\mathbb{R}^{d\\times 1}\\\\) là một điểm dữ liệu trong không gian \\\\(d\\\\) chiều. (_Chú ý: khác với các bài trước tôi thường dùng các vector hàng để mô tả dữ liệu, trong bài này tôi dùng vector cột để biểu diễn. Việc biểu diễn dữ liệu ở dạng hàng hay cột tùy thuộc vào từng bài toán, miễn sao cách biễu diễn toán học của nó khiến cho người đọc thấy dễ hiểu_).\n",
    "\n",
    "Giả sử thêm các nhãn tương ứng với từng điểm dữ liệu được lưu trong một vector hàng \\\\(\\mathbf{y} = [y_1, y_2, \\dots, y_N] \\in \\mathbb{R}^{1\\times d}\\\\), với \\\\(y_i = 1\\\\) nếu \\\\(\\mathbf{x}_i\\\\) thuộc class 1 (xanh) và \\\\(y_i = -1\\\\) nếu \\\\(\\mathbf{x}_i\\\\) thuộc class 2 (đỏ).\n",
    "\n",
    "Tại một thời điểm, giả sử ta tìm được boundary là đường phẳng có phương trình:\n",
    "\\\\[\n",
    "f_{\\mathbf{w}}(\\mathbf{x}) = w_1x_1 + w_2x_2 + \\dots + w_Nx_N + w_0 = \n",
    "\\\\]\n",
    "\\\\[\n",
    "=\\mathbf{w}^T\\mathbf{\\bar{x}} = 0\n",
    "\\\\]\n",
    "\n",
    "với \\\\(\\mathbf{\\bar{x}}\\\\) là điểm dữ liệu mở rộng bằng cách thêm phần tử 1 lên trước vector \\\\(\\mathbf{x}\\\\) tương tự như trong [Linear Regression](/2016/12/28/linearregression/). Và từ đây, khi nói \\\\(\\mathbf{x}\\\\), tôi cũng ngầm hiểu là điểm dữ liệu mở rộng.\n",
    "\n",
    "Để cho đơn giản, chúng ta hãy cùng làm việc với trường hợp mỗi điểm dữ liệu có số chiều \\\\(d = 2\\\\). Giả sử đường thẳng \\\\(w_1 x_1 + w_2 x_2 + w_0 = 0\\\\) chính là nghiệm cần tìm như Hình 2 dưới đây:\n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src =\"\\assets\\pla\\pla4.png\" align = \"center\" width = \"400\">\n",
    "</div> \n",
    "\n",
    "Nhận xét rằng các điểm nằm về cùng 1 phía so với đường thẳng này sẽ làm cho hàm số \\\\(f_{\\mathbf{w}}(\\mathbf{x})\\\\) mang cùng dấu. Chỉ cần đổi dấu của \\\\(\\mathbf{w}\\\\) nếu cần thiết, ta có thể giả sử các điểm nằm trong nửa mặt phẳng nền xanh mang dấu dương (+), các điểm nằm trong nửa mặt phẳng nền đỏ mang dấu ấm (-). Các dấu này cũng tương đương với nhãn \\\\(y\\\\) của mỗi class. Vậy nếu \\\\(\\mathbf{w}\\\\) là một nghiệm của bài toán Perceptron, với một điểm dữ liệu mới \\\\(\\mathbf{x}\\\\) chưa được gán nhãn, ta có thể xác định class của nó bằng phép toán đơn giản như sau:\n",
    "\\\\[\n",
    "\\text{label}(\\mathbf{x}) = 1 ~\\text{if}~ \\mathbf{w}^T\\mathbf{x} \\geq 0, \\text{otherwise} -1\n",
    "\\\\]\n",
    "\n",
    "Ngắn gọn hơn: \n",
    "\\\\[\n",
    "\\text{label}(\\mathbf{x}) = \\text{sgn}(\\mathbf{w}^T\\mathbf{x})\n",
    "\\\\]\n",
    "trong đó, \\\\(\\text{sgn}\\\\) là hàm xác định dấu, với giả sử rằng \\\\(\\text{sgn}(0) = 1\\\\).\n",
    "\n",
    "### Xây dựng hàm mất mát\n",
    "Tiếp theo, chúng ta cần xây dựng hàm mất mát với tham số \\\\(\\mathbf{w}\\\\) bất kỳ. Vẫn trong không gian hai chiều, giả sử đường thẳng \\\\(w_1x_1 + w_2x_2 + w_0 = 0\\\\) được cho như Hình 3 dưới đây:\n",
    "<div class=\"imgcap\">\n",
    "<img src =\"\\assets\\pla\\pla3.png\" align = \"center\" width = \"400\">\n",
    "</div> \n",
    "Trong trường hợp này, các điểm được khoanh tròn là các điểm bị misclassified (phân lớp lỗi). Điều chúng ta mong muốn là không có điểm nào bị misclassified. Hàm mất mát đơn giản nhất chúng ta nghĩ đến là hàm _đếm_ số lượng các điểm bị misclassied và tìm cách tối thiểu hàm số này:\n",
    "\\\\[\n",
    "J_1(\\mathbf{w}) = \\sum_{\\mathbf{x}_i \\in \\mathcal{M}} (-y_i\\text{sgn}(\\mathbf{w}^T\\mathbf{x_i}))\n",
    "\\\\]\n",
    "\n",
    "trong đó \\\\(\\mathcal{M}\\\\) là tập hợp các điểm bị misclassifed. Với mỗi điểm \\\\(\\mathbf{x}_i \\in \\mathcal{M}\\\\), vì điểm này bị misclassified nên \\\\(y_i\\\\) và \\\\(\\text{sgn}(\\mathbf{w}^T\\mathbf{x})\\\\) khác nhau và vì thế \\\\(-y_i\\text{sgn}(\\mathbf{w}^T\\mathbf{x_i}) = 1 \\\\). Vậy \\\\(J\\_1(\\mathbf{w})\\\\) chính là hàm _đếm_ số lượng các điểm bị misclassified. Khi hàm số này đạt giá trị nhỏ nhất bằng 0 thì ta không còn điểm nào bị misclassified. \n",
    "\n",
    "Một điểm quan trọng, hàm số này là rời rạc, không tính được đạo hàm theo \\\\(\\mathbf{w}\\\\) nên rất khó tối ưu. Chúng ta cần tìm một hàm mất mát khác để việc tối ưu khả thi hơn.\n",
    "\n",
    "Xét hàm mất mát sau đây: \n",
    "\\\\[\n",
    "J(\\mathbf{w}) = \\sum_{\\mathbf{x}_i \\in \\mathcal{M}} (-y_i\\mathbf{w}^T\\mathbf{x_i})\n",
    "\\\\]\n",
    "Hàm \\\\(J()\\\\) khác một chút với hàm \\\\(J\\_1()\\\\) ở việc bỏ đi hàm \\\\(\\text{sgn}\\\\). Nhận xét rằng khi một điểm misclassified \\\\(\\mathbf{x}_i\\\\) nằm càng xa boundary thì giá trị \\\\(-y_i\\mathbf{w}^T\\mathbf{x_i}\\\\) sẽ càng lớn, nghĩa là sự sai lệch càng lớn. Giá trị nhỏ nhất của hàm mất mát này cũng bằng 0 nếu không có điểm nào bị misclassifed. \n",
    "\n",
    "Tại một thời điểm, nếu chúng ta chỉ quan tâm tới các điểm bị misclassified thì hàm số \\\\(J(\\mathbf{w})\\\\) khả vi (tính được đạo hàm), vậy chúng ta có thể dụng [Gradient Descent](/2017/01/12/gradientdescent/) hoặc [Stochastic Gradient Descent (SGD)](/2017/01/16/gradientdescent2/#-stochastic-gradient-descent) để tối ưu hàm mất mát này. Với ưu điểm của SGD cho các bài toán [large-scale](/2017/01/12/gradientdescent/#large-scale), chúng ta sẽ làm theo thuật toán này. \n",
    "\n",
    "Với _một_ điểm dữ liệu \\\\(\\mathbf{x}_i\\\\) bị misclassified, hàm mất mát trở thành:\n",
    "\n",
    "\\\\[\n",
    "J(\\mathbf{w}; \\mathbf{x}_i; \\mathbf{y}_i) = -y_i\\mathbf{w}^T\\mathbf{x}_i\n",
    "\\\\]\n",
    "\n",
    "Đạo hàm tương ứng:\n",
    "\n",
    "\\\\[\n",
    "\\nabla\\_{\\mathbf{w}}J(\\mathbf{w}; \\mathbf{x}_i; \\mathbf{y}_i) = -y_i\\mathbf{x}_i\n",
    "\\\\]\n",
    "Vậy quy tắc cập nhật là:\n",
    "\\\\[\n",
    "\\mathbf{w} = \\mathbf{w} + \\eta y_i\\mathbf{x}_i\n",
    "\\\\]\n",
    "với \\\\(\\eta\\\\) là learning rate. \n",
    "\n",
    "Nhận xét rằng nếu \\\\(\\mathbf{w}\\\\) là nghiệm thì \\\\(\\eta\\mathbf{w}\\\\) cũng là nghiệm với \\\\(\\eta\\\\) là một số khác 0 bất kỳ. Vậy nếu \\\\(\\mathbf{w}\\_0\\\\) nhỏ gần với 0 và số vòng lặp đủ lớn, ta có thể coi như learning rate \\\\(\\eta = 1\\\\). Ta có một quy tắc cập nhật rất gọn là: \\\\(\\mathbf{w}\\_{t+1} = \\mathbf{w}_{t} + y\\_i\\mathbf{x}\\_i\\\\). Nói cách khác, với mỗi điểm \\\\(\\mathbf{x}_i\\\\) bị misclassifed, ta chỉ cần nhân điểm đó với nhãn \\\\(y_i\\\\) của nó, lấy kết quả cộng vào \\\\(\\mathbf{w}\\\\) ta sẽ được \\\\(\\mathbf{w}\\\\) mới.\n",
    "\n",
    "Ta có một quan sát nhỏ ở đây:\n",
    "\\\\[\n",
    "\\mathbf{w}\\_{t+1}^T\\mathbf{x}\\_i = (\\mathbf{w}\\_{t} + y_i\\mathbf{x}\\_i)^T\\mathbf{x}\\_{i} \\\\\\\n",
    "= \\mathbf{w}\\_{t}^T\\mathbf{x}\\_i + y\\_i \\|\\|\\mathbf{x}\\_i\\|\\|_2^2\n",
    "\\\\]\n",
    "\n",
    "Nếu \\\\(y\\_i = 1\\\\), vì \\\\(\\mathbf{x}\\_i\\\\) bị misclassifed nên \\\\(\\mathbf{w}\\_{t}^T\\mathbf{x}\\_i < 0\\\\). Cũng vì \\\\(y\\_i = 1\\\\) nên \\\\(y\\_i \\|\\|\\mathbf{x}\\_i\\|\\|\\_2^2 = \\|\\|\\mathbf{x}\\_i\\|\\|\\_2^2 \\geq 1\\\\) (vì \\\\(x\\_0 = 1\\\\)), nghĩa là \\\\(\\mathbf{w}\\_{t+1}^T\\mathbf{x}\\_i > \\mathbf{w}\\_{t}^T\\mathbf{x}\\_i\\\\). Lý giải bằng lời, \\\\(\\mathbf{w}\\_{t+1}\\\\) tiến về phía làm cho \\\\(\\mathbf{x}_i\\\\) được phân lớp đúng. Điều tương tự xảy ra nếu \\\\(y\\_i = -1\\\\).\n",
    "\n",
    "Đến đây, cảm nhận của chúng ta với thuật toán này là: cứ chọn đường boundary đi. Xét từng điểm một, nếu điểm đó bị misclassified thì tiến đường boundary về phía làm cho điểm đó được classifed đúng. Thoạt tiên có thể thấy rằng, mặc dù khi di chuyển đường boundary này, các điểm trước đó được classified đúng có thể lại bị misclassified, PLA vẫn đảm bảo sẽ hội tụ sau một số hữu hạn bước (tôi sẽ chứng minh việc này ở phía sau của bài viết). Tức là cuối cùng, ta sẽ tìm được đường phẳng phân chia hai lớp, miễn là hai lớp đó là linear separable. Đây cũng chính là lý do câu đầu tiên trong bài này tôi nói với các bạn là: \"Cứ làm đi, sai đâu sửa đó, cuối cùng sẽ thành công\".\n",
    "\n",
    "Tóm lại, thuật toán Perceptron có thể được viết như sau:\n",
    "\n",
    "### Tóm tắt PLA \n",
    "\n",
    "1. Chọn ngẫu nhiên một vector hệ số \\\\(\\mathbf{w}\\\\) với các phần tử gần 0.\n",
    "2. Duyệt ngẫu nhiên qua từng điểm dữ liệu \\\\(\\mathbf{x}_i\\\\):\n",
    "    * Nếu \\\\(\\mathbf{x}_i\\\\) được phân lớp đúng, tức \\\\(\\text{sgn}(\\mathbf{w}^T\\mathbf{x}) = y_i\\\\), chúng ta không cần làm gì.\n",
    "    * Nếu \\\\(\\mathbf{x}_i\\\\) bị misclassifed, cập nhật \\\\(\\mathbf{w}\\\\) theo công thức:\n",
    "    \\\\[\n",
    "    \\mathbf{w} = \\mathbf{w} + y\\_i\\mathbf{x}\\_i\n",
    "    \\\\]\n",
    "3. Kiểm tra xem có bao nhiêu điểm bị misclassifed. Nếu không còn điểm nào, dừng thuật toán. Nếu còn, quay lại bước 2.\n",
    "\n",
    "\n",
    "\n",
    "## Ví dụ trên Python\n",
    "\n",
    "## Chứng minh hội tụ\n",
    "\n",
    "Giả sử rằng \\\\(\\mathbf{w}^\\*\\\\) là một nghiệm của bài toán (ta có thể giả sử việc này được vì chúng ta đã có giả thiết hai class là linear separable - tức tồn tại nghiệm). Vì với mọi \\\\(\\alpha > 0\\\\), nên \\\\(\\alpha\\mathbf{w}^\\*\\\\) cũng là nghiệm của bài toán. Xét dãy số không âm \\\\( \\mathbf{u}\\_{\\alpha}(t) = \\|\\|\\mathbf{w}\\_{t} - \\alpha\\mathbf{w}^\\*\\|\\|\\_2^2\\\\). Với \\\\(\\mathbf{x}_i\\\\) là một điểm bị classified nếu dùng nghiệm \\\\(\\mathbf{w}\\_t\\\\) ta có: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\\\[\n",
    "\\begin{eqnarray}\n",
    "&&u\\_{\\alpha}(t+1) = \\|\\|\\mathbf{w}\\_{t+1} - \\alpha \\mathbf{w}^\\*\\|\\|\\_2^2 \\\\\\\n",
    "&=& \\|\\|\\mathbf{w}\\_{t} + y\\_i\\mathbf{x}\\_i - \\alpha\\mathbf{w}^\\*\\|\\|\\_2^2 \\\\\\\n",
    "&=& \\|\\|\\mathbf{w}\\_{t} -\\alpha\\mathbf{w}^\\*\\|\\|\\_2^2 + y\\_i^2\\|\\|\\mathbf{x}\\_i\\|\\|\\_2^2 + 2y\\_i\\mathbf{x}\\_i^T(\\mathbf{w} - \\mathbf{w}^*) \\\\\\\n",
    "&<& u\\_{\\alpha}(t) \\ + \\|\\|\\mathbf{x}\\_i\\|\\|\\_2^2 - 2y\\_i\\mathbf{x}\\_i^T \\mathbf{w}^\\*\n",
    "\\end{eqnarray}\n",
    "\\\\]\n",
    "\n",
    "\n",
    "Dấu nhỏ hơn ở dòng cuối là vì \\\\(y\\_i^2 = 1\\\\) và \\\\(2y\\_i\\mathbf{x}\\_i^T\\mathbf{w}\\_{t} < 0\\\\). Nếu ta đặt: \n",
    "\\\\[\n",
    "\\begin{eqnarray}\n",
    "\\beta^2 &=& \\max_{i=1, 2, \\dots, N}\\|\\mathbf{x}_i\\|_2^2 \\\\\\\n",
    "\\gamma &=& \\min_{i=1, 2, \\dots, N} y\\_i\\mathbf{x}\\_i^T\\mathbf{w}^\\*\n",
    "\\end{eqnarray}\n",
    "\\\\]\n",
    "\n",
    "và chọn \\\\(\\alpha = \\frac{\\beta^2}{\\gamma}\\\\), ta có:\n",
    "\\\\[\n",
    "0 \\leq u\\_{\\alpha}(t+1) < u\\_{\\alpha}(t) + \\beta^2 - 2\\alpha\\gamma = u\\_{\\alpha}(t) - \\beta^2\n",
    "\\\\]\n",
    "\n",
    "Điều này nghĩa là: nếu luôn luôn có các điểm bị misclassified thì dãy \\\\(u\\_{\\alpha}(t)\\\\) là dãy giảm, bị chặn dưới bởi 0, và phần tử sau kém phần tử trước ít nhất một lượng là \\\\(\\beta^2>0\\\\). Điều vô lý này chứng tỏ đến một lúc nào đó, không còn điểm nào bị misclassified. Nói cách khác, thuật toán PLA hội tụ sau một số hữu hạn bước. \n",
    "\n",
    "## Mô hình Neural Network đầu tiên\n",
    "Hàm số xác định class của Perceptron \\\\(\\text{label}(\\mathbf{x}) = \\text{sgn}(\\mathbf{w}^T\\mathbf{x})\\\\) có thể được mô tả như hình vẽ (được gọi là network) dưới đây:\n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src =\"\\assets\\pla\\pla_nn.png\" align = \"center\" width = \"800\">\n",
    "</div> \n",
    "\n",
    "Đầu vào của network \\\\(\\mathbf{x}\\\\) được minh họa bằng các node màu xanh lục với node \\\\(x_0\\\\) luôn luôn bằng 1. Tập hợp các node màu xanh lục được gọi là _Input layer_. Trong ví dụ này, tôi giả sử số chiều của dữ liệu \\\\(d = 4\\\\). Số node trong input layer luôn luôn là \\\\(d + 1\\\\) với môt node là 1 được thêm vào. Node \\\\(x_0 = 1\\\\) này đôi khi được ẩn đi. \n",
    "\n",
    "Các trọng số (_weights_) \\\\(w_0, w_1, \\dots, w_d\\\\) được gán vào các mũi tên đi tới node \\\\(z = \\sum\\_{i=0}^dw_ix_i = \\mathbf{w}^T\\mathbf{x}\\\\). Node \\\\(y = \\text{sgn}(z)\\\\) là _output_ của network. Ký hiệu hình chữa Z ngược màu xanh trong node \\\\(y\\\\) thể hiện đồ thị của hàm số \\\\(\\text{sgn}\\\\). \n",
    "\n",
    "Trong thuật toán PLA, ta phải tìm các weights trên các mũi tên sao cho với mỗi \\\\(\\mathbf{x}_i\\\\) ở tập các điểm dữ liệu đã biết được đặt ở Input layer, output của network này trùng với nhãn \\\\(y_i\\\\) tương ứng. \n",
    "\n",
    "Hàm số \\\\(y = \\text{sgn}(z)\\\\) còn được gọi là _activation function_. Đây chính là dạng đơn giản nhất của Neural Network.\n",
    "\n",
    "Để ý rằng nếu ta thay _activation function_ bởi \\\\(y = z\\\\), ta sẽ có Neural Network mô tả thuật toán Linear Regression như hình dưới. Với đường thẳng màu xanh thể hiện đồ thị hàm số \\\\(y = z\\\\). Các đường màu đen là các trục tọa độ, đôi khi cũng được lược bỏ.\n",
    "\n",
    "<div class=\"imgcap\">\n",
    "<img src =\"\\assets\\pla\\lr_nn.png\" align = \"center\" width = \"300\">\n",
    "</div> \n",
    "\n",
    "\n",
    "Các Neral Networks sau này có thể có nhiều node ở output tạo thành một _output layer_, hoặc có thể có thêm các layer trung gian giữa _input layer_ và _output layer_. Các layer trung gian đó được gọi là _hidden layer_. Khi biểu diễn các Networks lớn, người ta thường giản lược hình bên trái thành hình bên phải. Trong đó node \\\\(x_0 = 1\\\\) thường được ẩn đi. Node \\\\(z\\\\) cũng được ẩn đi và viết gộp vào trong node \\\\(y\\\\). Perceptron thường được vẽ dưới dạng đơn giản như hình bên phải. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Mô hình perceptron ở trên khá giống với một node nhỏ của dây thân kinh sinh học như hình sau đây:\n",
    "\n",
    "http://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_neuron.png\n",
    "(Nguồn: http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html)\n",
    "\n",
    "Dữ liệu từ nhiều dây thần kinh đi về một _cell nucleus_ và thông tin được tổng hợp và đưa ra ở output. Nhiều bộ phận như thế này kết hợp với nhau tạo nên hệ thần kinh sinh học. Chính vì vậy mà có tên Neural Networks trong Machine Learning. Đôi khi mạng này còn được gọi là Artificial Neural Networks (ANN) tức _hệ neuron nhân tạo_. \n",
    "\n",
    "## Thảo Luận\n",
    "\n",
    "## Tài liệu tham khảo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
