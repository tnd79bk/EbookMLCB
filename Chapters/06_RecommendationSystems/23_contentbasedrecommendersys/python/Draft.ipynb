{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng hàm mất mát\n",
    "\n",
    "Giả sử rằng số _users_ là \\\\(N\\\\), số _items_ là \\\\(M\\\\), _utility maxtrix_ được mô tả bởi ma trận \\\\(\\mathbf{Y}\\\\). Thành phần ở hàng thứ \\\\(m\\\\), cột thứ \\\\(n\\\\) của \\\\(\\mathbf{Y}\\\\) là _mức độ quan tâm_ (ở đây là số sao đã _rate_) của _user_ thứ \\\\(n\\\\) lên sản phẩm thứ \\\\(m\\\\) mà hệ thống đã thu thập được. Ma trận \\\\(Y\\\\) bị khuyết rất nhiều thành phần tương ứng với các giá trị mà hệ thống cần dự đoán. Thêm nữa, gọi \\\\(\\mathbf{R}\\\\) là ma trận _rated or not_ thể hiện việc một _user_ đã _rate_ một _item_ hay chưa. Cụ thể, \\\\(r\\_{ij}\\\\) bằng 1 nếu _item_ thứ \\\\(i\\\\) đã được _rated_ bởi _user_ thứ \\\\(j\\\\), bằng 0 trong trường hợp còn lại. \n",
    "\n",
    "\n",
    "\n",
    "**Mô hình tuyến tính:**\n",
    "\n",
    "Giả sử rằng ta có thể tìm được một mô hình cho mỗi _user_, minh hoạ bởi vector hệ số \\\\(\\mathbf{w}\\_i\\\\) sao cho _mức độ quan tâm_ của một _user_ tới một _item_ có thể tính được bằng một hàm tuyến tính:\n",
    "\n",
    "\\\\[\n",
    "y_{mn} = \\mathbf{x}_m ^T\\mathbf{w}_n + b ~~~~(1)\n",
    "\\\\]\n",
    "\n",
    "(_Chúng ta vẫn thường quen nhìn \\\\(\\mathbf{w}^T\\mathbf{x}\\\\) trong các mô hình tuyến tính trước đây, ở đây tôi đảo lại để thứ tự của chỉ số \\\\(m, n\\\\) thuận theo thứ tự khi xác định một phần tử của ma trận là (hàng, cột). Hai biểu thức là như nhau vì tích vô hướng của hai vector là một số vô hướng._)\n",
    "\n",
    "Một lần nữa, để cho các phép tính đơn giản hơn, chúng ta sẽ lại dùng đến [Bias trick](link), tức thêm 1 thành phần bằng 1 nữa vào mỗi feature vector và ghép \\\\(b\\\\) vào vector hệ số \\\\(\\mathbf{w}\\\\). Lúc này, ta sẽ viết lại biểu thức \\\\(1\\\\) dưới dạng:\n",
    "\n",
    "\\\\[\n",
    "y_{mn} = \\mathbf{x}_m^T\\mathbf{w}_n ~~~~(2)\n",
    "\\\\]\n",
    "\n",
    "_Chú ý rằng cả vector hệ số \\\\(\\mathbf{w}\\\\) và feature vector \\\\(\\mathbf{x}\\\\) trong \\\\((2)\\\\) là khác với chúng trong \\\\((1)\\\\) một chút_. \n",
    "\n",
    "Xét một _user_ thứ \\\\(n\\\\) bất kỳ, nếu ta coi training set là tập hợp các thành phần đã được _điền_ của \\\\(\\mathbf{y}_n\\\\), ta có thể xây dựng hàm mất mát (của \\\\(\\mathbf{w}_n\\\\)) tương tự như [Linear Regression] như sau: \n",
    "\n",
    "\\\\[\n",
    "\\mathcal{L}_n = \\frac{1}{2} \\sum_{m~:~ r_{mn} = 1}(\\mathbf{x}_m^T \\mathbf{w}_n - y_{mn})^2 + \\frac{\\lambda}{2} ~~~~~~ (2) \\|\\|\\mathbf{w}_n\\|\\|_2^2 \n",
    "\\\\]\n",
    "\n",
    "Trong đó, thành phần thứ hai là [regularization term](link) và \\\\(\\lambda\\\\) là một tham số dương. Trong thực hành, trung bình cộng của _lỗi_ thường được dùng, và _mất mát_ \\\\(\\mathcal{L}\\_n\\\\) được viết lại thành: \n",
    "\n",
    "\\\\[\n",
    "\\mathcal{L}\\_n = \\frac{1}{2s_n} \\sum_{m~:~ r_{mn} = 1}(\\mathbf{x}\\_m^T \\mathbf{w}\\_n - y_{mn})^2 + \\frac{\\lambda}{2s_n} \\|\\|\\mathbf{w}_n\\|\\|_2^2 \n",
    "\\\\]\n",
    "\n",
    "Trong đó \\\\(s_n\\\\) là số lượng các _items_ mà _user_ thứ \\\\(n\\\\) đã _rated_. Nói cách khác: \n",
    "\\\\[\n",
    "s_n = \\sum_{m=1}^M r_{mn},\n",
    "\\\\]\n",
    "là tổng các phần tử trên cột thứ \\\\(n\\\\) của ma trận _rated or not_ \\\\(\\mathbf{R}\\\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các bạn đã thấy _loss function_ này _quen_ chưa? \n",
    "\n",
    "Vì biểu thức _loss function_ chỉ phụ thuộc vào các _items_ đã được rated, ta có thể rút gọn nó bằng cách đặt \\\\(\\hat{\\mathbf{y}}_n\\\\) là _sub vector_ của \\\\(\\mathbf{y}\\\\) được xây dựng bằng cách _trích_ các thành phần _khác dấu ?_ ở cột thứ \\\\(n\\\\), tức đã được _rated_ bởi _user_ thứ \\\\(n\\\\) trong Utility Matrix \\\\(\\mathbf{Y}\\\\). Đồng thời, đặt \\\\(\\hat{\\mathbf{X}}_n\\\\) là _sub matrix_ của ma trận feature \\\\(\\mathbf{X}\\\\), được tạo bằng cách _trích_ các cột tương ứng với các _items_ đã được _rated_ bởi _user_ thứ \\\\(n\\\\). (_Xem ví dụ phía dưới để hiểu rõ hơn_). Khi đó, biểu thức hàm mất mát của mô hình cho _user_ thứ \\\\(n\\\\) được viết gọn thành: \n",
    "\n",
    "\\\\[\n",
    "\\mathcal{L}_n = \\frac{1}{2s_n} \\|\\|\\hat{\\mathbf{X}}_n^T\\mathbf{w}_n - \\hat{\\mathbf{y}}_n\\|\\|_2^2 + \\frac{\\lambda}{2s_n} \\|\\|\\mathbf{w}_n\\|\\|_2^2\n",
    "\\\\]\n",
    "\n",
    "Đây _chính xác_ là hàm mất mát của [Linear Regression] với regularization. Lời giải có thể trực tiếp thông qua pseudo inverse hoặc Stochastic Gradient Descent (SGD), hoặc Mini-batch GD. Dù bằng cách nào, ta cũng cần tính đạo hàm của hàm mất mát theo \\\\(\\mathbf{w}_n\\\\):\n",
    "\\\\[\n",
    "\\nabla_{\\mathbf{w}_n}\\mathcal{L}_n = \\frac{1}{s_n} \\hat{\\mathbf{X}}_n(\\hat{\\mathbf{X}}_n^T\\mathbf{w}_n - \\hat{\\mathbf{y}}_n) + \\frac{\\lambda}{s_n} \\mathbf{w}_n = \\frac{1}{s_n} (\\hat{\\mathbf{X}}_n \\hat{\\mathbf{X}}_n^T + \\lambda \\mathbf{I})\\mathbf{w}_n - \\hat{\\mathbf{X}}_n\\hat{\\mathbf{y}}_n\n",
    "\\\\]\n",
    "\n",
    "\n",
    "Với các bài toán nhỏ, nghiệm _chính xác_ có thể tìm được bằng cách giải phương trình đạo hàm của hàm mất mát theo \\\\(\\mathbf{w}_n\\\\) bằng 0. Như một bài tập nhỏ, bạn sẽ tìm được:\n",
    "\\\\[\n",
    "\\mathbf{w}_n = (\\hat{\\mathbf{X}}_n \\hat{\\mathbf{X}}_n^T + \\lambda \\mathbf{I})^{-1}\\hat{\\mathbf{X}}_n\\hat{\\mathbf{y}}_n ~~~~(3)\n",
    "\\\\]\n",
    "\n",
    "trong đó \\\\(\\mathbf{I}\\\\) là ma trận đơn vị. \n",
    "\n",
    "Với các bài toán lớn, tôi chọn cách thứ hai (Mini-batch GD) vì nó có khả năng làm việc với các bài toán _large scale_ với rất nhiều _users_ và _items_. Khi đó, công thức cập nhật cho Mini-batch GD có dạng: \n",
    "\\\\[\n",
    "\\mathbf{w}_n := \\mathbf{w}_n - \\eta \\left(\\frac{1}{s_n} (\\hat{\\mathbf{X}}_n \\hat{\\mathbf{X}}_n^T + \\lambda \\mathbf{I})\\mathbf{w}_n - \\hat{\\mathbf{X}}_n\\hat{\\mathbf{y}}_n\\right) ~~~~~ (4)\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu vẫn có điểm chưa hiểu, bạn đọc có thể xem ví dụ nhỏ dưới đây:\n",
    "\n",
    "### Ví dụ về hàm mất mát cho user E \n",
    "\n",
    "Quay trở lại với ví dụ trong hình 2, _feature matrix_ (đã thêm 1 vào hàng cuối cùng theo bias trick) cho các _items_ (mỗi cột tương ứng với một _item_) là: \n",
    "\\\\[\n",
    "\\mathbf{X} = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "0.99 & 0.91 & 0.95 & 0.01 & 0.03 \\\\\\\n",
    "0.02 & 0.11 & 0.05 &0.99 & 0.98 \\\\\\\n",
    "1 & 1 & 1 & 1 & 1\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\\\]\n",
    "\n",
    "Xét trường hợp của _user E_ với \\\\(n = 5\\\\), \\\\(\\mathbf{y}_5 = [5, ?, ?, 4, ?]^T \\Rightarrow \\mathbf{r}_5 = [1, 0, 0, 1, 0]^T\\\\). Vì _E_ mới chỉ _rated_ cho _items_ thứ nhất và thứ tư nên \\\\(s_5 = 2\\\\). Hơn nữa:\n",
    "\\\\[\n",
    "\\hat{\\mathbf{X}}_5 = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "0.99 &  0.01 \\\\\\\n",
    "0.02 & 0.99  \\\\\\\n",
    "1 & 1 \n",
    "\\end{matrix}\n",
    "\\right],\n",
    "\\hat{\\mathbf{y}}_5 = \\left[\n",
    "\\begin{matrix}\n",
    "5 \\\\\\\n",
    "4 \n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi đó, hàm mất mát cho hệ số tương ứng với _user E_ là: \n",
    "\\\\[\n",
    "\\mathcal{L}_5 = \\frac{1}{4} \\|\\|\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "0.99 &  0.02 & 1 \\\\\\\n",
    "0.01 & 0.99  & 1\n",
    "\\end{matrix}\n",
    "\\right]\\mathbf{w}_5 - \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "5 \\\\\\\n",
    "4\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\|\\|_2^2 + \\frac{\\lambda}{4} \\|\\|\\mathbf{w}_5\\|\\|_2^2\n",
    "\\\\]\n",
    "\n",
    "Và nghiệm theo công thức \\\\((3)\\\\) có thể tìm được như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.71446013]\n",
      " [ 2.36770762]\n",
      " [ 1.66070736]]\n",
      "[[ 1.00074599]\n",
      " [ 1.27099648]\n",
      " [ 1.10035562]\n",
      " [ 3.99759331]\n",
      " [ 3.95962703]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Xn = np.array([[0.99, 0.01], [0.02, 0.99], [1, 1]])\n",
    "yn = np.array([[1], [4]])\n",
    "lam = 0.001 \n",
    "I = np.eye(3)\n",
    "\n",
    "wn = np.linalg.inv(Xn.dot(Xn.T) + lam*I).dot(Xn.dot(yn))\n",
    "print wn\n",
    "\n",
    "X = np.array([[0.99, 0.91, 0.95, 0.01, 0.03],\\\n",
    "              [0.02, 0.11, 0.05, 0.99, 0.98],\\\n",
    "              [1, 1, 1, 1, 1]])\n",
    "print X.T.dot(wn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"collaborative-filtering\"></a>\n",
    "\n",
    "## Collaborative Filtering\n",
    "\n",
    "\n",
    "<a name=\"tai-lieu-tham-khao\"></a>\n",
    "\n",
    "## Tài liệu tham khảo\n",
    "\n",
    "[1] Chương 9\n",
    "\n",
    "[2] Recommendation systems - Machine Learning - Andrew Ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
